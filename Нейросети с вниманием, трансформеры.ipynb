{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Получение краткого содержания новости и ответов на вопросы с использованием предобученного BERT\n",
    "\n",
    "В сегодняшнем воркшопе мы с вами познакомимся с DeepPavlov. DeepPavlov - это библиотека обученных deep learning моделей для самых разных задач NLP, разработанная в МФТИ. В том числе там есть и множество моделей, предобученных на русскоязычных датасетах, что особенно отрадно для русского исследователя.\n",
    "\n",
    "Большинство моделей построены с использованием Tensorflow/Keras, но есть и модели для PyTorch.\n",
    "\n",
    "В этом воркшопе мы суммаризируем текст и попробуем получить ответы на вопросы по нему с использованием архитектуры BERT. Также посмотрим на то, как можно использовать предобученные эмбеддинги из DeepPavlov."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Установим DeepPavlov. Обратите внимание, что в настоящее время поддерживаются версии Python 3.6 и 3.7. Также, если вы работаете под Windows, у вас должен быть установлен Git и Visual Studio 2015/2017 с компилятором для С++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install deeppavlov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "У DeepPavlov немного нестандартная структура. Изначально вы не устанавливаете все модели, которые в нём имеются, ввиду их большого веса. После того, как вы установили фреймворк, вам нужно выбрать нужные модели, прописать для них конфигурации (можно использовать стандартные) и докачать их.\n",
    "\n",
    "Давайте для начала импортируем модуль и посмотрим на имеющиеся модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import deeppavlov as dp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Список возможных моделей вместе с файлами конфигурации хранится в `dp.configs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "frozenset({'classifiers',\n",
       "           'doc_retrieval',\n",
       "           'embedder',\n",
       "           'entity_extraction',\n",
       "           'faq',\n",
       "           'kbqa',\n",
       "           'morpho_syntax_parser',\n",
       "           'multitask',\n",
       "           'ner',\n",
       "           'odqa',\n",
       "           'ranking',\n",
       "           'regressors',\n",
       "           'relation_extraction',\n",
       "           'russian_super_glue',\n",
       "           'sentence_segmentation',\n",
       "           'spelling_correction',\n",
       "           'squad'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp.configs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модели разделены по задачам, для решения которых они предназначены. Давайте посмотрим, какие модели имеются для решения задачи суммаризации текста."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Struct' object has no attribute 'summarization'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfigs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummarization\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Struct' object has no attribute 'summarization'"
     ]
    }
   ],
   "source": [
    "dp.configs.summarization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Имеется 2 модели, отличащиеся только косметически. Фактически это один и тот же BERT, предобученный на задачах Masked Language Modeling (MLM) и Next Sentence Prediction (NSP): он предсказывает, какое из предложений наиболее вероятно должно следовать после текущего. В `bert_as_summarizer_with_init` мы можем сами задать, какое предложение будет выступать в качестве первого. В случае обработки новостей, например, логичнее всего в этом качестве использовать заголовок новости. В `bert_as_summarizer` в качестве первого предложения выступает первое предложение исходного текста. \n",
    "\n",
    "Мы будем использовать `bert_as_summarizer_with_init`, подавая в качестве опорного предложения заголовок новости.\n",
    "\n",
    "Но сперва давайте посмотрим, как выглядит конфигурационный файл для нашей модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"chainer\": {\n",
      "      \"in\": [\"texts\", \"init_sentences\"],\n",
      "      \"pipe\": [\n",
      "        {\n",
      "          \"class_name\": \"bert_as_summarizer\",\n",
      "          \"bert_config_file\": \"{DOWNLOADS_PATH}/bert_models/rubert_cased_L-12_H-768_A-12_v2/bert_config.json\",\n",
      "          \"pretrained_bert\": \"{DOWNLOADS_PATH}/bert_models/rubert_cased_L-12_H-768_A-12_v2/bert_model.ckpt\",\n",
      "          \"vocab_file\": \"{DOWNLOADS_PATH}/bert_models/rubert_cased_L-12_H-768_A-12_v2/vocab.txt\",\n",
      "          \"max_summary_length\": 100,\n",
      "          \"max_summary_length_in_tokens\": true,\n",
      "          \"lang\": \"ru\",\n",
      "          \"do_lower_case\": false,\n",
      "          \"max_seq_length\": 512,\n",
      "          \"in\": [\"texts\", \"init_sentences\"],\n",
      "          \"out\": [\"summarized_text\"]\n",
      "        }\n",
      "      ],\n",
      "      \"out\": [\"summarized_text\"]\n",
      "    },\n",
      "    \"metadata\": {\n",
      "      \"variables\": {\n",
      "        \"ROOT_PATH\": \"~/.deeppavlov\",\n",
      "        \"DOWNLOADS_PATH\": \"{ROOT_PATH}/downloads\",\n",
      "        \"MODELS_PATH\": \"{ROOT_PATH}/models\",\n",
      "        \"CONFIGS_PATH\": \"{DEEPPAVLOV_PATH}/configs\"\n",
      "      },\n",
      "      \"requirements\": [\n",
      "        \"{DEEPPAVLOV_PATH}/requirements/tf.txt\",\n",
      "        \"{DEEPPAVLOV_PATH}/requirements/bert_dp.txt\"\n",
      "      ],\n",
      "      \"download\": [\n",
      "        {\n",
      "          \"url\": \"http://files.deeppavlov.ai/deeppavlov_data/bert/rubert_cased_L-12_H-768_A-12_v2.tar.gz\",\n",
      "          \"subdir\": \"{DOWNLOADS_PATH}/bert_models\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "  \n",
      "  \n"
     ]
    }
   ],
   "source": [
    "with open(dp.configs.summarization.bert_as_summarizer_with_init, 'r') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В конфигурационном файле указываются пути для используемых моделей и словарей, описываются форматы входных и выходных данных а также указываются некоторые параметры задачи. Нас, например, интересуют `max_summary_length` и `max_summary_length_in_tokens`. В первом параметре указывается максимальная длинна саммари, которое должно получиться. Во втором - в чём она измеряется (в токенах или в предложениях).\n",
    "\n",
    "Для нас не очень удобно измерять длину в токенах, тем более, что в BERT зачастую 1 токен не всегда равен одному слову (слова могут биться на несколько токенов). Поэтому давайте изменим этот конфиг так, чтобы длина составляла, например, 5 предложений.\n",
    "\n",
    "Для этого скопируем файл и изменим соответствующие параметры."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"chainer\": {\n",
      "      \"in\": [\"texts\", \"init_sentences\"],\n",
      "      \"pipe\": [\n",
      "        {\n",
      "          \"class_name\": \"bert_as_summarizer\",\n",
      "          \"bert_config_file\": \"{DOWNLOADS_PATH}/bert_models/rubert_cased_L-12_H-768_A-12_v2/bert_config.json\",\n",
      "          \"pretrained_bert\": \"{DOWNLOADS_PATH}/bert_models/rubert_cased_L-12_H-768_A-12_v2/bert_model.ckpt\",\n",
      "          \"vocab_file\": \"{DOWNLOADS_PATH}/bert_models/rubert_cased_L-12_H-768_A-12_v2/vocab.txt\",\n",
      "          \"max_summary_length\": 5,\n",
      "          \"max_summary_length_in_tokens\": false,\n",
      "          \"lang\": \"ru\",\n",
      "          \"do_lower_case\": false,\n",
      "          \"max_seq_length\": 512,\n",
      "          \"in\": [\"texts\", \"init_sentences\"],\n",
      "          \"out\": [\"summarized_text\"]\n",
      "        }\n",
      "      ],\n",
      "      \"out\": [\"summarized_text\"]\n",
      "    },\n",
      "    \"metadata\": {\n",
      "      \"variables\": {\n",
      "        \"ROOT_PATH\": \"./deeppavlov\",\n",
      "        \"DOWNLOADS_PATH\": \"{ROOT_PATH}/downloads\",\n",
      "        \"MODELS_PATH\": \"{ROOT_PATH}/models\",\n",
      "        \"CONFIGS_PATH\": \"{DEEPPAVLOV_PATH}/configs\"\n",
      "      },\n",
      "      \"requirements\": [\n",
      "        \"{DEEPPAVLOV_PATH}/requirements/tf.txt\",\n",
      "        \"{DEEPPAVLOV_PATH}/requirements/bert_dp.txt\"\n",
      "      ],\n",
      "      \"download\": [\n",
      "        {\n",
      "          \"url\": \"http://files.deeppavlov.ai/deeppavlov_data/bert/rubert_cased_L-12_H-768_A-12_v2.tar.gz\",\n",
      "          \"subdir\": \"{DOWNLOADS_PATH}/bert_models\"\n",
      "        }\n",
      "      ]\n",
      "    }\n",
      "  }\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# посмотрим изменённый файл\n",
    "\n",
    "with open('bert_as_summarizer_with_init.json', 'r') as file:\n",
    "    print(file.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как уже говорилось выше, для использования модели её придётся скачать. Но предварительно придётся скачать и все необходимые зависимости, которые указаны в конфиге. \n",
    "\n",
    "Сделать это можно соответствующей командой:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m deeppavlov install bert_as_summarizer_with_init.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Построить модель, предварительно её скачав, можно при помощи функции `build_model`. При первом выполнении придётся подождать скачивания из репозитория."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-17 03:41:50.530 INFO in 'deeppavlov.download'['download'] at line 132: Skipped http://files.deeppavlov.ai/deeppavlov_data/bert/rubert_cased_L-12_H-768_A-12_v2.tar.gz download because of matching hashes\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\G0nZaleZ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\G0nZaleZ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package perluniprops to\n",
      "[nltk_data]     C:\\Users\\G0nZaleZ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package perluniprops is already up-to-date!\n",
      "[nltk_data] Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]     C:\\Users\\G0nZaleZ\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package nonbreaking_prefixes is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\deeppavlov\\core\\models\\tf_model.py:37: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\deeppavlov\\core\\models\\tf_model.py:222: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\deeppavlov\\core\\models\\tf_model.py:222: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\bert_dp\\modeling.py:92: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\bert_dp\\modeling.py:178: The name tf.variable_scope is deprecated. Please use tf.compat.v1.variable_scope instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\bert_dp\\modeling.py:418: The name tf.get_variable is deprecated. Please use tf.compat.v1.get_variable instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\bert_dp\\modeling.py:499: The name tf.assert_less_equal is deprecated. Please use tf.compat.v1.assert_less_equal instead.\n",
      "\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\bert_dp\\modeling.py:366: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\bert_dp\\modeling.py:680: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.Dense instead.\n",
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\tensorflow_core\\python\\layers\\core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.__call__` method instead.\n",
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\bert_dp\\modeling.py:283: The name tf.erf is deprecated. Please use tf.math.erf instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\deeppavlov\\models\\bert\\bert_as_summarizer.py:101: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-17 03:42:00.773 INFO in 'deeppavlov.models.bert.bert_as_summarizer'['bert_as_summarizer'] at line 102: [initializing model with Bert from C:\\Users\\G0nZaleZ\\PycharmProjects\\ws-deeppavlov\\deeppavlov\\downloads\\bert_models\\rubert_cased_L-12_H-768_A-12_v2\\bert_model.ckpt]\n"
     ]
    }
   ],
   "source": [
    "model = dp.build_model('bert_as_summarizer_with_init.json', download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Модель загружена!\n",
    "\n",
    "Давайте теперь познакомимся с текстом, с которым нам предстоит работать. Это новость о создании GPT-3: https://nplus1.ru/news/2020/05/29/gpt-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "title = \"Третье поколение алгоритма OpenAI научилось выполнять текстовые задания по нескольким примерам.\"\n",
    "\n",
    "text = \"\"\"\n",
    "Исследователи из OpenAI представили GPT-3 — алгоритм, который может выполнять разные задания по написанию текста на основе всего нескольких примеров. В новой версии используется та же архитектура, что и в предыдущем алгоритме GPT-2, однако разработчики увеличили количество используемых в модели параметров до 175 миллиардов, обучив модель на 570 гигабайтах текста. В итоге GPT-3 может отвечать на вопросы по прочитанному тексту, писать стихи, разгадывать анаграммы, решать простые арифметические примеры и даже переводить — и для этого ей нужно немного (от 10 до 100) примеров того, как именно это делать. Подробное описание работы алгоритма исследователи выложили на arXiv.org.\n",
    "\n",
    "Важное ограничение современных алгоритмов NLP (natural language processing) — зависимость от контекста: многие алгоритмы могут выполнять только те задачи, на выполнение которых они обучены. Например, если необходим алгоритм, который пишет стихи, его нужно обучить на большом корпусе стихов — желательно, в том стиле, в котором должно быть итоговое. Если обучение пройдет успешно, алгоритм сможет произвести что-то похожее на стих, но вот ответить на вопрос или составить список слов для кроссворда он уже не сможет.\n",
    "\n",
    "То, сколько данных понадобится для обучения NLP-алгоритма конкретной задаче, напрямую зависит от того, как алгоритм предобучен: если системе хорошо известны все требования грамматики языка, а генерировать осмысленные фразы он умеет изначально, то конкретно для обучения какой-то отдельной задаче нужно не так много данных. Задача, поэтому, сводится к тому, чтобы сделать предобученный NLP-алгоритм универсальным — таким, чтобы он фактически умел делать все, используя для обучения минимальное количество данных.\n",
    "\n",
    "Для решения этой задачи команда исследователей из компании OpenAI под руководством Тома Брауна (Tom Brown) представила GPT-3. Этот NLP-алгоритм основан на предыдущей версии, представленной в феврале прошлого года: GPT-2, одна из самых используемых и продвинутых NLP-моделей, обучена на 40 гигабайтах текста, а ее метазадача заключается в том, чтобы предсказывать следующее слово в тексте. Как и ее предшественник GPT, GPT-2 основана на архитектуре Transformer.\n",
    "\n",
    "Для обучения алгоритма исследователи собрали датасет из 570 гигабайтов текста, в который были включены данные проекта Common Crawl, вся Википедия, два датасета с книгами и вторая версия датасета WebText, содержащая тексты с веб-страниц (первую версию WebText использовали для обучения GPT-2). Исследователи обучили восемь разных моделей GPT-3: они отличались количеством параметров, которые модель устанавливала в ходе обучения (количество параметров, в свою очередь, зависело от количества слоев — при этом архитектура использовалась одна и та же). Внутри самой простой модели использовали 125 миллионов параметров, а в финальной GPT-3 — 175 миллиардов.\n",
    "\n",
    "Задача, которую GPT-3 нужно было выполнить, заключалась в ответе на вопрос или в выполнении задания. Это могло было быть, например, «написать стихотворение», «разобрать анаграмму» или «прочитать текст и ответить на вопрос». Предобученной GPT-3 для выполнения задания (всего заданий было 42), помимо формулировки задания, давали либо один пример, либо несколько примеров (классически от 10 до 100 — столько, сколько модели будет необходимо, хотя в некоторых заданиях модели хватало и пяти примеров).\n",
    "\n",
    "Несмотря на то, что точность каждого способа обучения модели возрастала с количеством определенных в модели параметров, обучение по нескольким примерам оказалось самым эффективным: по всем 42 заданиям точность при 175 миллиардах параметров составила почти 60 процентов. Например, при обучении на 64 примерах из датасета TriviaQA, который создан для обучения моделей понимать текст и отвечать на вопросы по прочитанному материалу, GPT-3 со 175 миллиардами параметров оказалась точна в 71,2 процента случаев — это чуть точнее, чем модель SOTA, которая обучена исключительно отвечать на вопросы по TriviaQA.\n",
    "\n",
    "По нескольким примерам предобученная GPT-3 может писать тексты на заданную тему, придумывать стихи в определенном стиле, разгадывать анаграммы, решать простые арифметические примеры, отвечать на вопросы по прочитанному тексту. Кроме того, модель может переводить на несколько языков: при сборе данных ученые не ограничили язык текстов, поэтому семь процентов всего датасета — тексты на иностранных языках, которые также используются моделью для перевода по нескольким примерам.\n",
    "\n",
    "Как и в случае с GPT-2, исследователи в препринте высказали свое беспокойство по поводу того, что разработанная ими модель может быть использована во вред — поэтому ее они пока что не предоставили. На странице разработчиков на GitHub можно найти кусок датасета и примеры заданий, которые использовались в работе.\n",
    "\n",
    "В последние годы OpenAI преуспел не только в NLP-алгоритмах: в прошлом году разработчики компании представили алгоритмы, которые могут придумывать новую музыку и собирать кубик Рубика.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получаем саммари, передав текст и первое предложение в нашу модель. Как обычно, модель может обрабатывать данные батчами, но в нашем случае это будет батч из одного примера."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Something went wrong while tokenizing\n"
     ]
    }
   ],
   "source": [
    "summary = model([text], [title])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Третье поколение алгоритма OpenAI научилось выполнять текстовые задания по нескольким примерам.\n",
      "Например, при обучении на 64 примерах из датасета TriviaQA, который создан для обучения моделей понимать текст и отвечать на вопросы по прочитанному материалу, GPT-3 со 175 миллиардами параметров оказалась точна в 71,2 процента случаев — это чуть точнее, чем модель SOTA, которая обучена исключительно отвечать на вопросы по TriviaQA.\n",
      "Для обучения алгоритма исследователи собрали датасет из 570 гигабайтов текста, в который были включены данные проекта Common Crawl, вся Википедия, два датасета с книгами и вторая версия датасета WebText, содержащая тексты с веб-страниц (первую версию WebText использовали для обучения GPT-2).\n",
      "В итоге GPT-3 может отвечать на вопросы по прочитанному тексту, писать стихи, разгадывать анаграммы, решать простые арифметические примеры и даже переводить — и для этого ей нужно немного (от 10 до 100) примеров того, как именно это делать.\n",
      "Предобученной GPT-3 для выполнения задания (всего заданий было 42), помимо формулировки задания, давали либо один пример, либо несколько примеров (классически от 10 до 100 — столько, сколько модели будет необходимо, хотя в некоторых заданиях модели хватало и пяти примеров).\n"
     ]
    }
   ],
   "source": [
    "# посмотрим результат\n",
    "\n",
    "for sent in summary[0]:\n",
    "    print(sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ещё одна из интересных задач NLP - ответы на вопросы по тексту (Question Answering). BERT тоже с этим успешно справляется, обучаясь ставить в тексте \"отметки\" в местах начала и конца ответа на вопрос.\n",
    "\n",
    "В DeepPavlov есть несколько предобученных BERT-ов для этой задачи на русском языке. Установим стандартный (здесь мы уже ничего в конфигах править не будем, просто скопируем стандартный):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m deeppavlov install squad_ru.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-17 03:46:33.280 INFO in 'deeppavlov.download'['download'] at line 132: Skipped http://files.deeppavlov.ai/deeppavlov_data/squad_model_ru_1.4_cpu_compatible.tar.gz download because of matching hashes\n",
      "INFO:deeppavlov.download:Skipped http://files.deeppavlov.ai/deeppavlov_data/squad_model_ru_1.4_cpu_compatible.tar.gz download because of matching hashes\n",
      "2020-11-17 03:46:41.440 INFO in 'deeppavlov.download'['download'] at line 132: Skipped http://files.deeppavlov.ai/embeddings/ft_native_300_ru_wiki_lenta_nltk_word_tokenize/ft_native_300_ru_wiki_lenta_nltk_word_tokenize.vec download because of matching hashes\n",
      "INFO:deeppavlov.download:Skipped http://files.deeppavlov.ai/embeddings/ft_native_300_ru_wiki_lenta_nltk_word_tokenize/ft_native_300_ru_wiki_lenta_nltk_word_tokenize.vec download because of matching hashes\n",
      "2020-11-17 03:46:41.477 INFO in 'deeppavlov.download'['download'] at line 132: Skipped http://files.deeppavlov.ai/embeddings/ft_native_300_ru_wiki_lenta_nltk_word_tokenize-char.vec download because of matching hashes\n",
      "INFO:deeppavlov.download:Skipped http://files.deeppavlov.ai/embeddings/ft_native_300_ru_wiki_lenta_nltk_word_tokenize-char.vec download because of matching hashes\n",
      "2020-11-17 03:46:41.488 INFO in 'deeppavlov.models.preprocessors.squad_preprocessor'['squad_preprocessor'] at line 310: SquadVocabEmbedder: loading saved tokens vocab from C:\\Users\\G0nZaleZ\\.deeppavlov\\models\\squad_model_ru\\emb\\vocab_embedder.pckl\n",
      "INFO:deeppavlov.models.preprocessors.squad_preprocessor:SquadVocabEmbedder: loading saved tokens vocab from C:\\Users\\G0nZaleZ\\.deeppavlov\\models\\squad_model_ru\\emb\\vocab_embedder.pckl\n",
      "2020-11-17 03:46:41.868 INFO in 'deeppavlov.models.preprocessors.squad_preprocessor'['squad_preprocessor'] at line 310: SquadVocabEmbedder: loading saved chars vocab from C:\\Users\\G0nZaleZ\\.deeppavlov\\models\\squad_model_ru\\emb\\char_vocab_embedder.pckl\n",
      "INFO:deeppavlov.models.preprocessors.squad_preprocessor:SquadVocabEmbedder: loading saved chars vocab from C:\\Users\\G0nZaleZ\\.deeppavlov\\models\\squad_model_ru\\emb\\char_vocab_embedder.pckl\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\deeppavlov\\core\\models\\tf_model.py:193: The name tf.train.AdadeltaOptimizer is deprecated. Please use tf.compat.v1.train.AdadeltaOptimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\deeppavlov\\core\\models\\tf_model.py:193: The name tf.train.AdadeltaOptimizer is deprecated. Please use tf.compat.v1.train.AdadeltaOptimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\deeppavlov\\core\\common\\check_gpu.py:29: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\deeppavlov\\core\\common\\check_gpu.py:29: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\deeppavlov\\models\\squad\\squad.py:224: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\deeppavlov\\models\\squad\\squad.py:224: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\tensorflow_core\\contrib\\cudnn_rnn\\python\\layers\\cudnn_rnn.py:342: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\tensorflow_core\\contrib\\cudnn_rnn\\python\\layers\\cudnn_rnn.py:342: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\tensorflow_core\\contrib\\cudnn_rnn\\python\\layers\\cudnn_rnn.py:345: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\tensorflow_core\\contrib\\cudnn_rnn\\python\\layers\\cudnn_rnn.py:345: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\deeppavlov\\core\\layers\\tf_layers.py:812: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "seq_dim is deprecated, use seq_axis instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\deeppavlov\\core\\layers\\tf_layers.py:812: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "seq_dim is deprecated, use seq_axis instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py:507: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "batch_dim is deprecated, use batch_axis instead\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\tensorflow_core\\python\\util\\deprecation.py:507: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "batch_dim is deprecated, use batch_axis instead\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\deeppavlov\\models\\squad\\utils.py:50: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\deeppavlov\\models\\squad\\utils.py:50: The name tf.AUTO_REUSE is deprecated. Please use tf.compat.v1.AUTO_REUSE instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\deeppavlov\\models\\squad\\utils.py:126: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\deeppavlov\\models\\squad\\utils.py:126: GRUCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.GRUCell, and will be replaced by that in Tensorflow 2.0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:559: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:559: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `layer.add_weight` method instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:575: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\tensorflow_core\\python\\ops\\rnn_cell_impl.py:575: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\deeppavlov\\models\\squad\\utils.py:139: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\deeppavlov\\models\\squad\\utils.py:139: The name tf.get_variable_scope is deprecated. Please use tf.compat.v1.get_variable_scope instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\deeppavlov\\models\\squad\\squad.py:203: The name tf.matrix_band_part is deprecated. Please use tf.linalg.band_part instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\deeppavlov\\models\\squad\\squad.py:203: The name tf.matrix_band_part is deprecated. Please use tf.linalg.band_part instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\deeppavlov\\models\\squad\\squad.py:212: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\deeppavlov\\models\\squad\\squad.py:212: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\deeppavlov\\core\\models\\tf_model.py:232: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\deeppavlov\\core\\models\\tf_model.py:232: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\deeppavlov\\core\\models\\tf_model.py:127: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\deeppavlov\\core\\models\\tf_model.py:127: The name tf.get_collection is deprecated. Please use tf.compat.v1.get_collection instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\deeppavlov\\core\\models\\tf_model.py:127: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\deeppavlov\\core\\models\\tf_model.py:127: The name tf.GraphKeys is deprecated. Please use tf.compat.v1.GraphKeys instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\tensorflow_core\\python\\ops\\clip_ops.py:172: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\tensorflow_core\\python\\ops\\clip_ops.py:172: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\tensorflow_core\\python\\training\\slot_creator.py:193: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\tensorflow_core\\python\\training\\slot_creator.py:193: Variable.initialized_value (from tensorflow.python.ops.variables) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use Variable.read_value. Variables in 2.X are initialized automatically both in eager and graph (inside tf.defun) contexts.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\deeppavlov\\models\\squad\\squad.py:93: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\deeppavlov\\models\\squad\\squad.py:93: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "2020-11-17 03:46:55.317 INFO in 'deeppavlov.core.models.tf_model'['tf_model'] at line 51: [loading model from C:\\Users\\G0nZaleZ\\.deeppavlov\\models\\squad_model_ru\\model]\n",
      "INFO:deeppavlov.core.models.tf_model:[loading model from C:\\Users\\G0nZaleZ\\.deeppavlov\\models\\squad_model_ru\\model]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\deeppavlov\\core\\models\\tf_model.py:54: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\g0nzalez\\venv_pure_deeppavlov_test\\lib\\site-packages\\deeppavlov\\core\\models\\tf_model.py:54: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\G0nZaleZ\\.deeppavlov\\models\\squad_model_ru\\model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from C:\\Users\\G0nZaleZ\\.deeppavlov\\models\\squad_model_ru\\model\n"
     ]
    }
   ],
   "source": [
    "# скачаем модель для QA\n",
    "\n",
    "model_qa = dp.build_model('squad_ru.json', download=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Попробуем найти в тексте ответы на следующие вопросы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\"Как называется алгоритм, занимающийся написанием текстов?\",\n",
    "             \"Сколько текста было в обучающем датасете?\",\n",
    "             \"Какое ограничение у современных алгоритмов NLP?\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Как называется алгоритм, занимающийся написанием текстов?\n",
      "GPT-3\n",
      "Сколько текста было в обучающем датасете?\n",
      "570 гигабайтов\n",
      "Какое ограничение у современных алгоритмов NLP?\n",
      "зависимость от контекста\n"
     ]
    }
   ],
   "source": [
    "# найдём ответ для каждого вопроса\n",
    "\n",
    "for q in questions:\n",
    "    print(q)\n",
    "    answer = model_qa([text], [q])[0][0]\n",
    "    print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также в Deeppavlov есть предтренированные на больших датасетах эмбеддинги, которые вы можете использовать в своей работе подобно тому, как мы использовали однажды предварительно натренированный word2vec.\n",
    "\n",
    "Помимо таких \"тяжеловесов\", как BERT, есть, например, и более простые модели. Например, ELMO - это эмбеддинг, построенный при помощи LSTM, который может назначать слову разные вектора в зависимости от контекста, в котором оно используется. Есть разные его варианты, обученные на разных датасетах. Например, на русской Википедии. Скачаем его (тоже воспользовавшись стандартным конфигом)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# устанавливаем зависимости\n",
    "\n",
    "!python -m deeppavlov install elmo_ru_wiki.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-11-17 03:50:25.447 INFO in 'deeppavlov.download'['download'] at line 132: Skipped http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-wiki_600k_steps.tar.gz download because of matching hashes\n",
      "INFO:deeppavlov.download:Skipped http://files.deeppavlov.ai/deeppavlov_data/elmo_ru-wiki_600k_steps.tar.gz download because of matching hashes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "# качаем и строим модель\n",
    "\n",
    "elmo_emb = dp.build_model('elmo_ru_wiki.json', download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# можно \"закодировать\" все необходимые слова\n",
    "\n",
    "texts = ['Получаем эмбеддинг для слов этого предложения.',\n",
    "         'Конечно, можно использовать несколько предложений. В качестве текста. Токенайзер справится.',\n",
    "         'Слово']\n",
    "\n",
    "embs = elmo_emb(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.52713317,  0.04464398,  0.02200007, ...,  0.39243197,\n",
       "         -0.07147413,  1.7890232 ],\n",
       "        [-0.5409472 ,  0.07987181,  0.6760626 , ...,  0.22213186,\n",
       "          0.30867967, -0.42369205],\n",
       "        [ 0.15041974,  0.6983181 ,  0.25723952, ...,  0.10992923,\n",
       "         -0.5024707 ,  0.24565211],\n",
       "        ...,\n",
       "        [ 0.19936937,  0.5070391 , -0.1287077 , ..., -0.08864522,\n",
       "          0.3586684 ,  0.35104832],\n",
       "        [-0.25707465,  0.67701054,  0.21267587, ...,  0.16995324,\n",
       "         -0.8818088 ,  0.78075826],\n",
       "        [-0.6920643 , -0.449081  , -1.3315636 , ..., -0.08664615,\n",
       "         -0.13441339, -0.23009208]], dtype=float32),\n",
       " array([[-0.3380093 ,  0.38195705, -1.3128859 , ...,  0.92860615,\n",
       "          0.35657376,  0.6304276 ],\n",
       "        [ 0.18537113,  0.10103117, -0.75799304, ...,  0.31713006,\n",
       "         -0.39594996,  0.10783564],\n",
       "        [-0.8533196 , -0.69447803, -1.2852573 , ..., -0.14508472,\n",
       "         -1.047672  ,  0.1478431 ],\n",
       "        ...,\n",
       "        [-0.42853767,  1.0058446 , -0.16986546, ...,  0.38513976,\n",
       "         -0.57933205,  0.00430027],\n",
       "        [-0.71470845, -0.90436876,  0.1697484 , ...,  0.6166826 ,\n",
       "         -0.99255604,  0.2002173 ],\n",
       "        [-0.7306882 , -0.27039933, -0.8482108 , ..., -0.08664615,\n",
       "         -0.13441339, -0.23009208]], dtype=float32),\n",
       " array([[-0.07167736, -0.7923741 , -0.1028476 , ..., -1.0585611 ,\n",
       "         -0.3749748 ,  0.7346015 ]], dtype=float32)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# на выходе получаем список эмбеддингов\n",
    "# т.к. все они разной длины (для разных предложений)\n",
    "# они упаковываются в обычный список, а не np-массив\n",
    "\n",
    "embs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 2560)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# посмотрим на размерность эмбеддингов\n",
    "\n",
    "embs[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Данный эмбеддинг кодирует в том числе и знаки препинания. В частности, можно видеть, что все предложения заканчиваются одним вектором, соответствующим точке. \n",
    "\n",
    "Но в последнем предложении всего 1 вектор, т.к. оно состоит всего лишь из одного слова. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2560)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embs[2].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Эмбеддинг очень объёмный: 2560 измерений. Однако такие сложные вектора могут помочь хорошо уловить контекст.\n",
    "\n",
    "Обучить подобную модель в домашних условиях достаточно сложно, но зато вы можете пользоваться ей в своих разработках. Просто прогоняйте через неё текст перед тем, как отправить на обучение или обработку в свою модель для NLP-задачи, и это значительно улучшит качество вашей работы. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DeepPavlov (Python 3.9)",
   "language": "python",
   "name": "deeppavlov_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
