{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdb272c2-82ca-47e2-b766-7afcf46e8654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gym\n",
      "  Downloading gym-0.26.2.tar.gz (721 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m721.7/721.7 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m36m-:--:--\u001b[0m\n",
      "  Installing build dependencies ... \u001b[done\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /opt/anaconda3/lib/python3.12/site-packages (from gym) (1.26.4)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from gym) (2.2.1)\n",
      "Collecting gym_notices>=0.0.4 (from gym)\n",
      "  Downloading gym_notices-0.1.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Downloading gym_notices-0.1.0-py3-none-any.whl (3.3 kB)\n",
      "Building wheels for collected packages: gym\n",
      "  Building wheel for gym (pyproject.toml) ... \u001b[?25done\n",
      "\u001b[?25h  Created wheel for gym: filename=gym-0.26.2-py3-none-any.whl size=827730 sha256=fa390e77f09f1787957c600199659c3a71dc96138d21e3dd272fb3e698f361cb\n",
      "  Stored in directory: /Users/uliamalueva/Library/Caches/pip/wheels/95/51/6c/9bb05ebbe7c5cb8171dfaa3611f32622ca4658d53f31c79077\n",
      "Successfully built gym\n",
      "Installing collected packages: gym_notices, gym\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [gym]\n",
      "\u001b[1A\u001b[2KSuccessfully installed gym-0.26.2 gym_notices-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "856d24b9-fca6-45a6-9f33-edccf9a3588d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n"
     ]
    }
   ],
   "source": [
    "# не забудьте установить gym, если он у вас ещё не установлен\n",
    "# при помощи pip install gym\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# фиксируем сид\n",
    "np.random.seed(17)\n",
    "tf.random.set_seed(17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9522f7d2-7e4c-4f88-a9b0-beb3807e98cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.03450748, -0.03390269,  0.00577445, -0.01319201], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")  # создадим среду\n",
    "env.reset(seed=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6e9305ee-6431-43fe-bdf5-a9c8a318548d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38], [4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38], (4,), float32)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "025fc718-43c7-4478-b5ac-5d1db2a62575",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "799f41f3-78ee-4b8c-ac8b-c1b93ee67b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_actions = env.action_space.n\n",
    "state_shape = env.observation_space.shape\n",
    "\n",
    "inputs = layers.Input(shape=(4,))\n",
    "common = layers.Dense(64, activation=\"relu\")(inputs)\n",
    "common2 = layers.Dense(64, activation=\"relu\")(common)\n",
    "action = layers.Dense(2, activation=\"softmax\")(common2)\n",
    "critic = layers.Dense(1)(common2)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=[action, critic])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ac0a32f9-163c-4ba4-86bf-e462b570bac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# в качестве оптимизатора будем по стандарту использовать ADAM\n",
    "optimizer = keras.optimizers.Adam(learning_rate=0.01)\n",
    "\n",
    "# в качестве функции потерь - Huber loss\n",
    "# она похожа на MSE, но оканчивается линейрой функцией, а не квадратичной\n",
    "# это придаст чуть больше стабильности нашему процессу обучения\n",
    "huber_loss = keras.losses.Huber()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b5a9dba-a955-4dfa-be25-91174f5d5dff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10, running reward: 11.19\n",
      "Episode: 20, running reward: 12.13\n",
      "Episode: 30, running reward: 12.69\n",
      "Episode: 40, running reward: 13.54\n",
      "Episode: 50, running reward: 15.33\n",
      "Episode: 60, running reward: 14.85\n",
      "Episode: 70, running reward: 15.48\n",
      "Episode: 80, running reward: 26.67\n",
      "Episode: 90, running reward: 28.87\n",
      "Episode: 100, running reward: 37.05\n",
      "Episode: 110, running reward: 38.04\n",
      "Episode: 120, running reward: 40.40\n",
      "Episode: 130, running reward: 48.90\n",
      "Episode: 140, running reward: 63.83\n",
      "Episode: 150, running reward: 83.46\n",
      "Episode: 160, running reward: 136.74\n",
      "Episode: 170, running reward: 103.49\n",
      "Episode: 180, running reward: 66.92\n",
      "Episode: 190, running reward: 44.75\n",
      "Episode: 200, running reward: 31.13\n",
      "Episode: 210, running reward: 22.42\n",
      "Episode: 220, running reward: 17.37\n",
      "Episode: 230, running reward: 14.39\n",
      "Episode: 240, running reward: 12.89\n",
      "Episode: 250, running reward: 11.79\n",
      "Episode: 260, running reward: 11.25\n",
      "Episode: 270, running reward: 11.18\n",
      "Episode: 280, running reward: 11.00\n",
      "Episode: 290, running reward: 11.17\n",
      "Episode: 300, running reward: 13.09\n",
      "Episode: 310, running reward: 14.59\n",
      "Episode: 320, running reward: 19.17\n",
      "Episode: 330, running reward: 21.38\n",
      "Episode: 340, running reward: 27.93\n",
      "Episode: 350, running reward: 41.68\n",
      "Episode: 360, running reward: 83.83\n",
      "Episode: 370, running reward: 106.12\n",
      "Solved at episode 379!\n"
     ]
    }
   ],
   "source": [
    "# зададим discount-фактор для награды\n",
    "gamma = 0.995\n",
    "\n",
    "# как долго длится эпизод\n",
    "max_steps_per_episode = 10000\n",
    "\n",
    "# минимальная машинная точность\n",
    "eps = np.finfo(np.float32).eps.item()\n",
    "\n",
    "# выделим списки для хранения значений\n",
    "# вероятности действий, критика и награды\n",
    "action_probs_history = []\n",
    "critic_value_history = []\n",
    "rewards_history = []\n",
    "\n",
    "\n",
    "running_reward = 0\n",
    "episode_count = 0\n",
    "\n",
    "while True:\n",
    "    # сбрасываем состояние к изначальному и записываем его\n",
    "    state = env.reset()\n",
    "    # Убедимся, что state является numpy array правильной формы\n",
    "    if isinstance(state, tuple):\n",
    "        state = state[0]  # для новых версий gym\n",
    "    state = np.array(state, dtype=np.float32)\n",
    "    \n",
    "    episode_reward = 0\n",
    "    \n",
    "    # проводим раунд игры, записывая работу нашей нейросети в GradientTape\n",
    "    with tf.GradientTape() as tape:\n",
    "        # идём по раунду шагами\n",
    "        for timestep in range(1, max_steps_per_episode):\n",
    "            env.render()  # для графического отображения окна с игрой\n",
    "\n",
    "            state = tf.convert_to_tensor(state)\n",
    "            state = tf.expand_dims(state, 0)\n",
    "\n",
    "            # зная состояние, модель должна нам предсказать действие \n",
    "            # и поправку от критика\n",
    "            action_probs, critic_value = model(state)\n",
    "            \n",
    "            # поправку критика сразу сохраним\n",
    "            critic_value_history.append(critic_value[0, 0])\n",
    "\n",
    "            # выберем действие, исходя из предсказанных вероятностей\n",
    "            action = np.random.choice(num_actions, p=np.squeeze(action_probs))\n",
    "            \n",
    "            # вероятность выбранного действия сохраняем в историю\n",
    "            # можно сразу и прологарифмировать, как того требует наш лосс\n",
    "            action_probs_history.append(tf.math.log(action_probs[0, action]))\n",
    "\n",
    "            # говорим среде выполнить выбранное действие\n",
    "            # и получаем от неё следующее состояние и награду\n",
    "            # если игра окончена, среда вернут done=True\n",
    "            result = env.step(action)\n",
    "            state, reward, done = result[0], result[1], result[2]\n",
    "\n",
    "            # Убедимся, что state является numpy array\n",
    "            state = np.array(state, dtype=np.float32)\n",
    "            \n",
    "            # записываем полученную награду\n",
    "            rewards_history.append(reward)\n",
    "            \n",
    "            episode_reward += reward\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        # пересчитываем running_reward с затуханием\n",
    "        running_reward = 0.05 * episode_reward + (1 - 0.05) * running_reward\n",
    "\n",
    "        # награда на каждом шаге игры (то, что нам и должен предсказать критик)\n",
    "        # определяется как суммарная награда от этого и всех последующих шагов (с затуханием)\n",
    "        # считаем их все в 1 проход и добавляем в returns\n",
    "        returns = []\n",
    "        discounted_sum = 0\n",
    "        for r in rewards_history[::-1]:\n",
    "            discounted_sum = r + gamma * discounted_sum\n",
    "            returns.insert(0, discounted_sum)\n",
    "\n",
    "        # нормализуем их для повышения стабильности обучения\n",
    "        returns = np.array(returns)\n",
    "        returns = (returns - np.mean(returns)) / (np.std(returns) + eps)\n",
    "        returns = returns.tolist()\n",
    "\n",
    "        # осталось посчитать значения функций потерь, чтобы затем осуществить шаг градиентного спуска\n",
    "        history = zip(action_probs_history, critic_value_history, returns)\n",
    "        actor_losses = []\n",
    "        critic_losses = []\n",
    "        for log_prob, value, ret in history:\n",
    "            # на каждом шаге считаем лосс для actor и critic\n",
    "            # у первого это просто награда (за вычетом поправки от критика), \n",
    "            # умноженная на логарифмическую вероятность действия\n",
    "            diff = ret - value\n",
    "            actor_losses.append(-log_prob * diff)  # actor loss\n",
    "\n",
    "            # критик же оканчивается обычной регрессией:\n",
    "            # он должен уметь предсказывать ожидаемую награду на каждом шаге\n",
    "            critic_losses.append(\n",
    "                loss(tf.expand_dims(value, 0), tf.expand_dims(ret, 0))\n",
    "            )\n",
    "\n",
    "        # теперь можем осуществить стандартный шаг градиентного спуска\n",
    "        # чтобы запустить его из одной точки, сложим все лоссы\n",
    "        # т.к. при суммировании градиент от каждого слагаемого отправится\n",
    "        # только в направлении данного слагаемого, это то, что нужно\n",
    "        loss_value = sum(actor_losses) + sum(critic_losses)\n",
    "        grads = tape.gradient(loss_value, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # перед следующим эпизодом очистим все списки\n",
    "        action_probs_history.clear()\n",
    "        critic_value_history.clear()\n",
    "        rewards_history.clear()\n",
    "\n",
    "    episode_count += 1\n",
    "    \n",
    "    # каждые 10 эпизодов будем выводить running reward\n",
    "    if episode_count % 10 == 0:\n",
    "        print(f\"Episode: {episode_count}, running reward: {running_reward:.2f}\")\n",
    "\n",
    "    # если running reward стал больше 195, цель достигнута\n",
    "    if running_reward > 195:\n",
    "        print(f\"Solved at episode {episode_count}!\")\n",
    "        break\n",
    "\n",
    "# саму среду в конце нужно закрыть\n",
    "# чтобы рендер прекратился и можно было дальше работать в питоне\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1ce5d9a6-3700-46ce-a6f0-93dcbc4bf9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Исходное состояние: [-0.03200178  0.04104776 -0.00531698 -0.0443652 ]\n"
     ]
    }
   ],
   "source": [
    "# Сбросим состояние среды и выполним 1 шаг\n",
    "state = env.reset()\n",
    "# Для новых версий gym может возвращаться tuple, берем только состояние\n",
    "if isinstance(state, tuple):\n",
    "    state = state[0]\n",
    "state = np.array(state, dtype=np.float32)\n",
    "\n",
    "print(\"Исходное состояние:\", state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "328d9a32-e133-4bb8-8bbc-1bf88b24601d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Состояние как тензор: tf.Tensor([[-0.03200178  0.04104776 -0.00531698 -0.0443652 ]], shape=(1, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Преобразуем в тензор\n",
    "state_tensor = tf.convert_to_tensor(state)\n",
    "state_tensor = tf.expand_dims(state_tensor, 0)\n",
    "\n",
    "print(\"Состояние как тензор:\", state_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "525c31d2-955e-4369-9cdc-4b8a1c8553c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Вероятности действий: [[0.8601593  0.13984066]]\n",
      "Сумма вероятностей: 0.99999994\n"
     ]
    }
   ],
   "source": [
    "# Получаем вероятности действий от модели\n",
    "action_probs, _ = model(state_tensor)\n",
    "\n",
    "print(\"\\nВероятности действий:\", action_probs.numpy())\n",
    "print(\"Сумма вероятностей:\", np.sum(action_probs.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4e11f390-52cc-4eef-99a5-87539a3df9e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Вероятность действия 0: 0.8602\n",
      "Вероятность действия 1: 0.1398\n"
     ]
    }
   ],
   "source": [
    "# Посмотрим на конкретные значения\n",
    "probs = action_probs.numpy()[0]\n",
    "print(f\"Вероятность действия 0: {probs[0]:.4f}\")\n",
    "print(f\"Вероятность действия 1: {probs[1]:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a15819-c70a-4f58-af29-ff14e1b85c04",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
